from dotenv import load_dotenv
from langchain.agents import Tool
from langchain.chains import ConversationChain
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools import DuckDuckGoSearchResults
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from utils import extract_links
from web_crawling import get_dynamic_page_text
from web_search import search_serper_links

import os
import streamlit as st

load_dotenv()

# ì‚¬ìš©ìì˜ inputì„ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰í•  tool ì„¸íŒ…
search_tool = DuckDuckGoSearchResults()
tools = [
    Tool(
        name="web_search",
        func=search_tool.run,
        description="ì‚¬ìš©ìì˜ ë²•ë¥  ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê³µê³µê¸°ê´€ ì›¹ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
    )
]

llm = ChatOpenAI(model="gpt-4.1", temperature=0.2) # GPT LLM êµ¬ì„±
memory = ConversationBufferMemory(return_messages=True) # ë©”ëª¨ë¦¬ ì„¤ì • (ì´ì „ ëŒ€í™” ê¸°ì–µ)
prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ í•œêµ­ì˜ ë²•ë¥  ìƒë‹´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 
ì•„ë˜ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ì…ë‹ˆë‹¤:

{history}

---

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸:
{input}

ë²•ì  ì´ìŠˆë¥¼ ë¶„ë¥˜í•˜ê³ , êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆì„ ì„¤ëª…í•´ì£¼ì„¸ìš” (ê¸°ê´€ëª…, ì ˆì°¨, í•„ìš”ì„œë¥˜, ì œì¶œë§í¬ ë“±).
""")
output_parser = StrOutputParser() # ì¶œë ¥ íŒŒì„œ

# ConversationChainìœ¼ë¡œ ëŒ€í™”í˜• ì±— êµ¬ì„±
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=prompt,
    output_parser=output_parser,
    verbose=True
)

# Streamlit ì•±
st.set_page_config(page_title="ë²•ë¥  ìƒë‹´ ì±—ë´‡", layout="wide")
st.markdown("## ğŸ§‘â€âš–ï¸ ë²•ë¥  ì±—ë´‡ (LangChain + GPT)")

# ëŒ€í™” ì…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for role, msg in st.session_state.chat_history:
    with st.chat_message("user" if role == "ğŸ§‘â€ğŸ’¼ ì§ˆë¬¸" else "assistant"):
        st.markdown(msg if role != "ğŸ§‘â€ğŸ’¼ ì§ˆë¬¸" else f"**{msg}**")

# ì…ë ¥ì´ ìˆëŠ” ê²½ìš° â†’ ë‹µë³€ ë°›ê³  ì¶œë ¥
if user_input:
    with st.chat_message("user"):
        st.markdown(f"**{user_input}**")

    with st.chat_message("assistant"):
        with st.spinner("GPTê°€ í•´ê²° ë°©ì•ˆì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            # 1ï¸âƒ£ GPTë¡œ ë²•ì  ì´ìŠˆ ë¶„ë¥˜
            issue_classification_prompt = f"""
            ë‹¤ìŒ ë¬¸ì¥ì€ ì–´ë–¤ ë²•ì  ë¬¸ì œì— í•´ë‹¹í•©ë‹ˆê¹Œ?
            ê°€ëŠ¥í•œ ë¶„ë¥˜ ì¤‘ ì •í™•íˆ í•˜ë‚˜ë§Œ ê³¨ë¼ì£¼ì„¸ìš”:

            ë¶„ë¥˜: ì„ê¸ˆì²´ë¶ˆ, ë¶€ë‹¹í•´ê³ , ê°œì¸ì •ë³´ ìœ ì¶œ, ê³„ì•½ì„œ ë¯¸ì‘ì„±, ëª…ì˜ˆí›¼ì†, ê°€ì •í­ë ¥, ê¸°íƒ€

            ë¬¸ì¥: {user_input}
            """
            issue_type_msg = llm.invoke(issue_classification_prompt)
            issue_type = issue_type_msg.content.strip() 
            st.markdown(f"ğŸ§  ê°ì§€ëœ ë²•ì  ì´ìŠˆ: **{issue_type}**")

            # 2ï¸âƒ£ Serper ê²€ìƒ‰ (ë¬¸ì œ ìœ í˜• ê¸°ë°˜)
            SERPER_API_KEY = os.getenv("SERPER_API_KEY")  # .envì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
            query = f"{issue_type} ê´€ë ¨ ì‹ ê³  ì ˆì°¨ site:moel.go.kr OR site:gov.kr OR site:minwon.go.kr"
            
            urls = search_serper_links(query, api_key=SERPER_API_KEY)
            first_url = urls[0] if urls else None

            # for url in urls: # í¬ë¡¤ë§ í›„ë³´êµ° ë””ë²„ê¹…ìš© UI
            #     st.markdown(f'í¬ë¡¤ë§ í›„ë³´êµ°: {url}')

            page_text = get_dynamic_page_text(first_url) if first_url else "[ê´€ë ¨ í˜ì´ì§€ ì—†ìŒ]"

            # 3ï¸âƒ£ GPTì—ê²Œ í•´ê²° ë°©ì•ˆ ìš”ì²­
            final_prompt = f"""
            ê°ì§€ëœ ë²•ì  ì´ìŠˆ: {issue_type}

            ì‚¬ìš©ìì˜ ì§ˆë¬¸: {user_input}

            ë‹¤ìŒì€ í•´ë‹¹ ì´ìŠˆì— ëŒ€í•œ ê³µì‹ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ìƒìœ„ ê²°ê³¼ 1ê°œì˜ ì‹¤ì œ í˜ì´ì§€ ë³¸ë¬¸ì…ë‹ˆë‹¤.

            **ë‹¨, ì´ ë³¸ë¬¸ì—ëŠ” ìƒë‹¨ ë©”ë‰´/ê³ ê°ì„¼í„°/ì €ì‘ê¶Œ ë“±ì˜ ë¶€ê°€ í…ìŠ¤íŠ¸ë„ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ**,  
            ì‹¤ì œ ë²•ë¥ ì  í•´ê²° ë°©ë²•ê³¼ ì ˆì°¨, ì‹ ê³  ê¸°ê´€, ì²˜ë¦¬ ìˆœì„œ, ì£¼ì˜ì‚¬í•­, ì„œë¥˜ ì•ˆë‚´ ë“± **í•µì‹¬ ì •ë³´ë§Œ ì„ ë³„í•˜ì—¬** ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ëŠ” í•´ê²° ë°©ë²•ì„ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

            ê³µì‹ ë³¸ë¬¸:
            {page_text}

            ìœ„ ë‚´ìš©ì„ ì¢…í•©í•´ ì‚¬ìš©ìê°€ ì·¨í•´ì•¼ í•  ë²•ì  ëŒ€ì‘ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
            """
            response = conversation.predict(input=final_prompt)

            # ğŸ§¾ GPT ë‹µë³€ ì¶œë ¥
            st.markdown(response)

            # ğŸ” ì‘ë‹µì—ì„œ ë§í¬ ëª¨ë‘ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ + ì¼ë°˜ URL + HTML ë§í¬)
            urls = extract_links(response)
            if urls:
                with st.expander("ğŸ”— ê´€ë ¨ ë§í¬ ë³´ê¸°"):
                    for url in urls[:3]:
                        st.markdown(f"- [{url}]({url})")
            else: st.info("ğŸ”— GPT ì‘ë‹µì— ë§í¬ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            with st.expander("ğŸ“ ì°¸ê³ í•œ í˜ì´ì§€ ë³¸ë¬¸ ë³´ê¸°"):
                st.markdown(f'ì°¸ê³ í•œ í˜ì´ì§€ ë§í¬: {first_url}')
                st.markdown(page_text if page_text else "_ë³¸ë¬¸ ì—†ìŒ_")

    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    st.session_state.chat_history.append(("ğŸ§‘â€ğŸ’¼ ì§ˆë¬¸", user_input))
    st.session_state.chat_history.append(("ğŸ¤– GPT", response))