from dotenv import load_dotenv
from langchain.chains import ConversationChain
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

import streamlit as st

load_dotenv()

llm = ChatOpenAI(model="gpt-4-0125-preview", temperature=0.2) # GPT LLM êµ¬ì„±
memory = ConversationBufferMemory(return_messages=True) # ë©”ëª¨ë¦¬ ì„¤ì • (ì´ì „ ëŒ€í™” ê¸°ì–µ)
prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ í•œêµ­ì˜ ë²•ë¥  ìƒë‹´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 
ì•„ë˜ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ì…ë‹ˆë‹¤:

{history}

---

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸:
{input}

ë²•ì  ì´ìŠˆë¥¼ ë¶„ë¥˜í•˜ê³ , êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆì„ ì„¤ëª…í•´ì£¼ì„¸ìš” (ê¸°ê´€ëª…, ì ˆì°¨, í•„ìš”ì„œë¥˜, ì œì¶œë§í¬ ë“±).
""")
output_parser = StrOutputParser() # ì¶œë ¥ íŒŒì„œ

# ConversationChainìœ¼ë¡œ ëŒ€í™”í˜• ì±— êµ¬ì„±
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=prompt,
    output_parser=output_parser,
    verbose=True
)

# Streamlit ì•±
st.markdown("## ğŸ§‘â€âš–ï¸ ë²•ë¥  ì±—ë´‡ (LangChain + GPT)")

# ëŒ€í™” ì…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for role, msg in st.session_state.chat_history:
    with st.chat_message("user" if role == "ğŸ§‘â€ğŸ’¼ ì§ˆë¬¸" else "assistant"):
        st.markdown(msg if role != "ğŸ§‘â€ğŸ’¼ ì§ˆë¬¸" else f"**{msg}**")

# ì…ë ¥ì´ ìˆëŠ” ê²½ìš° â†’ ë‹µë³€ ë°›ê³  ì¶œë ¥
if user_input:
    with st.chat_message("user"):
        st.markdown(f"**{user_input}**")

    with st.chat_message("assistant"):
        with st.spinner("GPTê°€ í•´ê²° ë°©ì•ˆì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            response = conversation.predict(input=user_input)
            st.markdown(response)

    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    st.session_state.chat_history.append(("ğŸ§‘â€ğŸ’¼ ì§ˆë¬¸", user_input))
    st.session_state.chat_history.append(("ğŸ¤– GPT", response))